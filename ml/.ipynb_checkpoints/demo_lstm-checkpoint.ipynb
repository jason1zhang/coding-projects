{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5919ea-397a-46ed-be5c-07443359c760",
   "metadata": {},
   "source": [
    "# This notebook used generated random samples to simulate time series data, and use LSTM model to do the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73eb9a9d-5c3b-4ab6-b015-78a492c7bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eee223f-c316-4eba-9ef3-be6bc2b056cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced data set generation\n",
    "def generate_imbalanced_data(num_samples, ratio):\n",
    "    random.seed(32)\n",
    "    data = []\n",
    "    positive_samples = int(num_samples * ratio)\n",
    "    positive_count = 0\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        feature1 = random.random()\n",
    "        feature2 = random.random()\n",
    "\n",
    "        # generate positive samples first\n",
    "        if positive_count < positive_samples:\n",
    "            label = 1\n",
    "            positive_count += 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        data.append([feature1, feature2, label])\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5807d25c-0772-4904-bc0f-39f13c78b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8370e235-f7c6-4e43-9759-2d8ed9ce30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "ratio = 0.0001\n",
    "data = generate_imbalanced_data(500000, ratio)\n",
    "\n",
    "X = [d[:-1] for d in data]\n",
    "y = [d[-1] for d in data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732ee2e4-1c2d-49d0-936a-10f380ed3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 2\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0904efaa-ef69-4706-9f4b-e2436d187f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantize model, loss function and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549ce1a6-e89f-4ae0-82db-4976e213fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0001\n",
      "Epoch [2/10], Loss: 0.0001\n",
      "Epoch [3/10], Loss: 0.0001\n",
      "Epoch [4/10], Loss: 0.0001\n",
      "Epoch [5/10], Loss: 0.0001\n",
      "Epoch [6/10], Loss: 0.0001\n",
      "Epoch [7/10], Loss: 0.0001\n",
      "Epoch [8/10], Loss: 0.0001\n",
      "Epoch [9/10], Loss: 0.0001\n",
      "Epoch [10/10], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # adjust the input shape to be (batch_size, seq_len, input_size), namely [64, 1, 2),\n",
    "        # which is the expected input shape for LSTM model.\n",
    "        # In this case, LSTM is applied to a simulated time series data, with time step of 1.\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17abda80-abb2-4262-8312-9c080f9eaf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 99.992%\n",
      "confusion matrix:\n",
      "[[99992     0]\n",
      " [    8     0]]\n",
      "\n",
      " false negative records:\n",
      "Input: [[0.9534358978271484, 0.20167024433612823]], True Label: 1, predicted label: 0\n",
      "Input: [[0.15954242646694183, 0.020099591463804245]], True Label: 1, predicted label: 0\n",
      "Input: [[0.6887012124061584, 0.9517923593521118]], True Label: 1, predicted label: 0\n",
      "Input: [[0.49170637130737305, 0.954414427280426]], True Label: 1, predicted label: 0\n",
      "Input: [[0.04933225363492966, 0.038266588002443314]], True Label: 1, predicted label: 0\n",
      "Input: [[0.9671679139137268, 0.9225612878799438]], True Label: 1, predicted label: 0\n",
      "Input: [[0.7805687189102173, 0.3158128559589386]], True Label: 1, predicted label: 0\n",
      "Input: [[0.09090518206357956, 0.06240416318178177]], True Label: 1, predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    fn_records = []\n",
    "    correct = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "        # Find false negative records\n",
    "        for i, (label, pred) in enumerate(zip(labels, predicted)):\n",
    "            if label == 1 and pred == 0:\n",
    "                fn_records.append((inputs[i].tolist(), label.item(), pred.item()))\n",
    "\n",
    "        # count correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    total = len(y_true)\n",
    "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\n false negative records:\")\n",
    "    for record in fn_records:\n",
    "        print(f\"Input: {record[0]}, True Label: {record[1]}, predicted label: {record[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5713f-1793-450e-bf11-fd6e6b735aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fedd9d3-7cbd-49d2-ab99-4252eef41ac8",
   "metadata": {},
   "source": [
    "# The program below is generated by Kimi.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25ab4525-c790-4ebc-9a39-7d0a36d87601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Test Accuracy: 48.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置随机种子以复现结果\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 生成模拟数据\n",
    "def generate_data(seq_length, num_sequences):\n",
    "    # 假设有两个特征和一个标签\n",
    "    features = np.random.rand(num_sequences, seq_length, 2)\n",
    "    labels = np.random.choice([0, 1], num_sequences)\n",
    "\n",
    "    # 转换为PyTorch张量\n",
    "    features_tensor = torch.FloatTensor(features)\n",
    "    labels_tensor = torch.LongTensor(labels)\n",
    "    \n",
    "    return features_tensor, labels_tensor\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size)\n",
    "        \n",
    "        # 前向传播LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 取最后一个时间步的输出\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 超参数\n",
    "input_size = 2  # 输入特征的维度\n",
    "hidden_size = 50  # LSTM层的隐藏状态维度\n",
    "num_layers = 1  # LSTM层的数量\n",
    "output_size = 1  # 输出层的维度（二分类）\n",
    "seq_length = 10  # 序列长度\n",
    "num_sequences = 1000  # 序列数量\n",
    "num_epochs = 100  # 训练轮数\n",
    "batch_size = 64  # 批次大小\n",
    "learning_rate = 0.01  # 学习率\n",
    "\n",
    "# 生成数据\n",
    "features, labels = generate_data(seq_length, num_sequences)\n",
    "\n",
    "# 划分数据集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 构建数据加载器\n",
    "train_loader = DataLoader(dataset=TensorDataset(train_features, train_labels), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(test_features, test_labels), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，确保标签是浮点型并且是二维的\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "            \n",
    "# 测试模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)  # 确保模型输出是原始logits\n",
    "        # 确保标签是二维的\n",
    "        labels = labels.view(-1, 1)  # 将标签转换为二维张量\n",
    "        total += labels.size(0)\n",
    "        # 预测类别（直接比较logits与0）\n",
    "        predicted = outputs > 0  # logits大于0的为1，否则为0\n",
    "        # 计算正确预测的数量\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f4d9d-087e-465b-ae24-8610ea3d7564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
