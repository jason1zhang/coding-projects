{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5919ea-397a-46ed-be5c-07443359c760",
   "metadata": {},
   "source": [
    "# This notebook used generated random samples to simulate time series data, and use LSTM model to do the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73eb9a9d-5c3b-4ab6-b015-78a492c7bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eee223f-c316-4eba-9ef3-be6bc2b056cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced data set generation\n",
    "def generate_imbalanced_data(num_samples, ratio):\n",
    "    random.seed(32)\n",
    "    data = []\n",
    "    positive_samples = int(num_samples * ratio)\n",
    "    positive_count = 0\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        feature1 = random.random()\n",
    "        feature2 = random.random()\n",
    "\n",
    "        # generate positive samples first\n",
    "        if positive_count < positive_samples:\n",
    "            label = 1\n",
    "            positive_count += 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        data.append([feature1, feature2, label])\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5807d25c-0772-4904-bc0f-39f13c78b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8370e235-f7c6-4e43-9759-2d8ed9ce30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "ratio = 0.0001\n",
    "data = generate_imbalanced_data(500000, ratio)\n",
    "\n",
    "X = [d[:-1] for d in data]\n",
    "y = [d[-1] for d in data]\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732ee2e4-1c2d-49d0-936a-10f380ed3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 2\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0904efaa-ef69-4706-9f4b-e2436d187f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantize model, loss function and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549ce1a6-e89f-4ae0-82db-4976e213fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0001\n",
      "Epoch [2/10], Loss: 0.0001\n",
      "Epoch [3/10], Loss: 0.0001\n",
      "Epoch [4/10], Loss: 0.0001\n",
      "Epoch [5/10], Loss: 0.0001\n",
      "Epoch [6/10], Loss: 0.0001\n",
      "Epoch [7/10], Loss: 0.0001\n",
      "Epoch [8/10], Loss: 0.0001\n",
      "Epoch [9/10], Loss: 0.0001\n",
      "Epoch [10/10], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # adjust the input shape to be (batch_size, seq_len, input_size), namely [64, 1, 2),\n",
    "        # which is the expected input shape for LSTM model.\n",
    "        # In this case, LSTM is applied to a simulated time series data, with time step of 1.\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17abda80-abb2-4262-8312-9c080f9eaf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 99.992%\n",
      "confusion matrix:\n",
      "[[99992     0]\n",
      " [    8     0]]\n",
      "\n",
      " false negative records:\n",
      "Input: [[0.9534358978271484, 0.20167024433612823]], True Label: 1, predicted label: 0\n",
      "Input: [[0.15954242646694183, 0.020099591463804245]], True Label: 1, predicted label: 0\n",
      "Input: [[0.6887012124061584, 0.9517923593521118]], True Label: 1, predicted label: 0\n",
      "Input: [[0.49170637130737305, 0.954414427280426]], True Label: 1, predicted label: 0\n",
      "Input: [[0.04933225363492966, 0.038266588002443314]], True Label: 1, predicted label: 0\n",
      "Input: [[0.9671679139137268, 0.9225612878799438]], True Label: 1, predicted label: 0\n",
      "Input: [[0.7805687189102173, 0.3158128559589386]], True Label: 1, predicted label: 0\n",
      "Input: [[0.09090518206357956, 0.06240416318178177]], True Label: 1, predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    fn_records = []\n",
    "    correct = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "        # Find false negative records\n",
    "        for i, (label, pred) in enumerate(zip(labels, predicted)):\n",
    "            if label == 1 and pred == 0:\n",
    "                fn_records.append((inputs[i].tolist(), label.item(), pred.item()))\n",
    "\n",
    "        # count correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    total = len(y_true)\n",
    "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
    "\n",
    "    # confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\n false negative records:\")\n",
    "    for record in fn_records:\n",
    "        print(f\"Input: {record[0]}, True Label: {record[1]}, predicted label: {record[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5713f-1793-450e-bf11-fd6e6b735aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
